{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tarfile\n",
    "from nets import inception_v4 as inception\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy : yy + short_edge, xx : xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (299, 299, 3))\n",
    "    return resized_img.astype(np.float32)\n",
    "\n",
    "def load_all_image(path, files):\n",
    "    data = [load_image(path + \"/\" + file) for file in files]\n",
    "    labels = [1 if \"dog\" in file else 0 for file in files]\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def download_and_uncompress_tarball(tarball_url, dataset_dir):\n",
    "    checkpoints_dir = '/tmp/checkpoints'\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    filename = tarball_url.split('/')[-1]\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(filepath + ' exits')\n",
    "    else:\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dataset_dir)\n",
    "    print('extractall', filepath)\n",
    "\n",
    "def get_batchs(files, batch_size):\n",
    "    length = len(files)\n",
    "    total_batch = (length + batch_size - 1) // batch_size\n",
    "    for i in range(total_batch):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if end > length:\n",
    "            end = length\n",
    "        yield start, end, files[start: end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Image : Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_dir/inception_v4_2016_09_09.tar.gz exits\n",
      "extractall checkpoints_dir/inception_v4_2016_09_09.tar.gz\n",
      "load checkpoint success\n",
      "process train batch  0 2\n",
      "process test batch  0 2\n",
      "data saved\n"
     ]
    }
   ],
   "source": [
    "pretrain_data_url = 'http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz'\n",
    "checkpoints_dir = 'checkpoints_dir'\n",
    "download_and_uncompress_tarball(pretrain_data_url, checkpoints_dir)\n",
    "\n",
    "train_files = os.listdir('data/train')[0:2]\n",
    "test_files = os.listdir('data/test')[0:2]\n",
    "train_features = np.zeros((len(train_files), 1536))\n",
    "train_labels = np.zeros(len(train_files))\n",
    "test_features = np.zeros((len(test_files), 1536))\n",
    "test_labels = np.zeros(len(test_files))\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "pretrain_graph = tf.Graph()\n",
    "with pretrain_graph.as_default():\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, imge_size, imge_size, 3), name='inputs')\n",
    "    \n",
    "    with slim.arg_scope(inception.inception_v4_arg_scope()):\n",
    "        logits, end_points = inception.inception_v4(inputs, is_training=False)\n",
    "    \n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    PreLogitsFlatten = end_points['PreLogitsFlatten']\n",
    "    \n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'inception_v4.ckpt'),\n",
    "        slim.get_model_variables('InceptionV4'))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_fn(sess)\n",
    "        print(\"load checkpoint success\")\n",
    "        \n",
    "        for start, end, files in get_batchs(train_files, batch_size):\n",
    "            data, labels = load_all_image('data/train', files)\n",
    "            prob, logits = sess.run([probabilities, PreLogitsFlatten], feed_dict={inputs:data})\n",
    "            print(\"process train batch \", start, end)\n",
    "            \n",
    "            train_features[start:end] = logits\n",
    "            train_labels[start:end] = labels\n",
    "            \n",
    "        for start, end, files in get_batchs(test_files, batch_size):\n",
    "            data, labels = load_all_image('data/test', files)\n",
    "            prob, logits = sess.run([probabilities, PreLogitsFlatten], feed_dict={inputs:data})\n",
    "            print(\"process test batch \", start, end)\n",
    "            \n",
    "            test_features[start:end] = logits\n",
    "            test_labels[start:end] = labels\n",
    "            \n",
    "np.save(\"train_features.npy\", train_features)\n",
    "np.save(\"train_labels.npy\", train_labels)\n",
    "np.save(\"test_features.npy\", test_features)\n",
    "np.save(\"test_labels.npy\", test_labels)\n",
    "print(\"data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = np.load(\"train_features.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "test_features = np.load(\"test_features.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dog_cat_model(inputs, is_training=True, scope=\"dog_cat_model\"):\n",
    "    with tf.variable_scope(scope, 'dog_cat_model', [inputs]):\n",
    "        end_points = {}\n",
    "        \n",
    "        with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "\n",
    "            net = slim.fully_connected(inputs, 1024, scope='fc1')\n",
    "            end_points['fc1'] = net\n",
    "\n",
    "            net = slim.dropout(net, 0.8, is_training=is_training)\n",
    "\n",
    "            net = slim.fully_connected(net, 1024, scope='fc2')\n",
    "            end_points['fc2'] = net\n",
    "\n",
    "            net = slim.fully_connected(net, 1, activation_fn=None, scope='logits')\n",
    "            end_points['logits'] = net\n",
    "\n",
    "            predictions = tf.nn.softmax(net, name='predictions')\n",
    "            end_points['predictions'] = tf.nn.sigmoid(logits, name='predictions')\n",
    "            return predictions, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  16.1181\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'dog_cat_model'\n",
    "\n",
    "dog_cat_graph = tf.Graph()\n",
    "with dog_cat_graph.as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float64, shape=(None, 1536), name=\"inputs\")\n",
    "    labels = tf.placeholder(tf.float64, shape=(None), name=\"labels\")\n",
    "\n",
    "    predictions, end_points = create_dog_cat_model(inputs, is_training=True)\n",
    "    loss = tf.losses.log_loss(labels, predictions)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        loss_val,_ = sess.run([loss, train_op], feed_dict={inputs:train_features, labels:train_labels})\n",
    "        print(\"Training Loss \", loss_val)\n",
    "\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
